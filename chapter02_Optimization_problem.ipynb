{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chapter02_Optimization_problem.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1YGiAGcZOhsmfqc568LOpweJHmGCSviWD",
      "authorship_tag": "ABX9TyNsVsHFNlqnw0UZVMbG572P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanyoseob/lecture_optimization/blob/main/chapter02_Optimization_problem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8S_-aU2V-Wbj"
      },
      "source": [
        "# Optimization problem\n",
        "---\n",
        "Below equation is a standard form of an objective function to solve an optimization problem.\n",
        ">$x^* = \\arg \\min_{x} F(x) + \\lambda R(x)$,\n",
        "\n",
        "where $F(\\cdot)$ denotes the fidelity term, $R(\\cdot)$ is the regularization term (or penalty term), and $\\lambda$ is hyper-parameter to balance between the fidelity term $F(x)$ and the regularization term $R(x)$.\n",
        "\n",
        "Commonly, the fidelity term $F(x)$ is formulated by\n",
        "\n",
        ">$F(x) = \\frac{1}{2} || Ax - y ||_2^2$,\n",
        "\n",
        "where $A$ denotes a system matrix and $y$ is measurement collected from the system matrix $A$.\n",
        "In previous Chapter 1, we used a Gaussian-blur kernel as the system matrix $A$.\n",
        "\n",
        "In addition, the regularization term $R(x)$ is used like total variation (TV), low-rankness, etc.\n",
        "\n",
        "1. Total variation (TV) panelty\n",
        ">$R(x) = |D_{\\textrm{x}}(x)| + |D_{\\textrm{y}}(x)|$,\n",
        "\n",
        "  where $D_{\\textrm{x}}(\\cdot)$ and $D_{\\textrm{y}}(\\cdot)$ are differentiation along the $\\textrm{x}$-axis and $\\textrm{y}$-axis, respectively.\n",
        "\n",
        "2. Low-rankness\n",
        ">$R(x) = \\textrm{RANK}(x) < k$,\n",
        "\n",
        "  where $k$ is the rank-constraint of $x$.\n",
        "\n",
        "\n",
        "Below formulation is the standard form of the objective function we are dealing with.\n",
        "\n",
        ">$x^* = \\arg \\min_{x} \\frac{1}{2} || A x - y ||_2^2 + \\lambda R(x)$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFanhuAlBOpt"
      },
      "source": [
        "# Various optimization solvers\n",
        "---\n",
        "\n",
        "To find a optimal solve $x^*$ satisfying above objective function, we will implement various optimization solvers such as \n",
        "\n",
        "1. [Gradient descent method](https://en.wikipedia.org/wiki/Gradient_descent)\n",
        "2. [Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method)\n",
        "3. [Conjugate gradient method](https://en.wikipedia.org/wiki/Conjugate_gradient_method)\n",
        "4. [Iterative Shrinkage-Thresholding Algorithm](http://www.cs.cmu.edu/afs/cs/Web/People/airg/readings/2012_02_21_a_fast_iterative_shrinkage-thresholding.pdf)\n",
        "5. [Fast Iterative Shrinkage-Thresholding Algorithm](http://www.cs.cmu.edu/afs/cs/Web/People/airg/readings/2012_02_21_a_fast_iterative_shrinkage-thresholding.pdf)\n",
        "6. [Alternating Direction Method of Multipliers](https://web.stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDtqwdZoX8G1"
      },
      "source": [
        "# Next\n",
        "---\n",
        "\n",
        "[Next chapter](https://colab.research.google.com/drive/1tlOUw92mmyWnUW2f72DfeR0EK8RhkKtT?usp=sharing), we will cover the gradient descent method and implement a 1D toy-example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkLWRe9JS4qo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}